Using traditional machine learning classifiers, such as Gradient Boosted Trees, Logistic Regression and Random Forest, forced us to train one model for every kind of activity, resulting in 51 classifiers. Each of those decides in a one-versus-rest decision if an activity is present or not. 
First approaches with Scikit-Learn's \texttt{OneVsRestClassifier} were not successful, because this class only expects a single classifier as input, forcing us to use the same set of hyperparameters for all 51 individual problems. This restriction proved to be problematic, because the optimal values of hyperparaters vary a lot depending on the kind of input data, particularly if large class imbalances are present.
 
Another problem is the XGBoost classifier with \texttt{gpu\_hist} tree method, which proved to be the most successful individual model. It uses a very efficient approximate histogram evaluation on the GPU to find the best splits in the boosting algorithm. With help of this method we can train large ensembles of trees in a matter of minutes instead of hours on the CPU. Unfortunately, XGBoost does not release the allocated memory as good as it should, causing out-of-memory errors when using the OneVsRestClassifier.
This encouraged us to implement our own kind of one-vs-rest classifier that offers, as the name implies, greater flexibility. 

\subsection{FlexOneVsRestClassifier}

The \texttt{FlexOneVsRestClassifier} is a classifier with Scikit-Learn syntax that wraps a set of individual single-label classifiers and allows to use them for multi-label classification. The constructor either expects a single classifier, which is copied for every kind of label, or a dictionary of classifiers. These don't even have to be of the same kind, but can be a mixture of different models. This allows us for example to predict the activity \enquote{lying down} with an XGBoost classifier and the activity \enquote{elevator}, that has a much larger class imbalance, with Logistic Regression.

The most sophisticated method of the FlexOneVsRestClassifier is its \texttt{tune\_hyperparam} method that can be used to tune the hyperparameters of the individual classifiers. It expects a dataset with labels, a number of starting points and iterations and a set of hyperparameters with bounds to optimize (for example \texttt{max\_depth}:(6, 12) allows trees with a maximum depth between six and twelve).  

\subsection{Ensemble Classifier}

blub