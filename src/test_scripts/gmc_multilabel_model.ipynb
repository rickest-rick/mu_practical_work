{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "from tensorflow import keras\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.utils import class_weight\n",
    "\n",
    "from gmc_loss import GmcLoss, get_coexist_counts\n",
    "from data_handling import load_user_data, split_features_labels, \\\n",
    "    user_train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sensitivity(y_true, y_pred):\n",
    "    # mask where y_true is nan\n",
    "    is_not_nan = tf.logical_not(tf.math.is_nan(y_true))\n",
    "    y_true = tf.boolean_mask(y_true, is_not_nan)\n",
    "    y_pred = tf.boolean_mask(y_pred, is_not_nan)\n",
    "    # return true positive ratio\n",
    "    true_positives = keras.backend.sum(tf.math.round(keras.backend.clip(\n",
    "        y_true * y_pred, 0, 1)))\n",
    "    possible_positives = keras.backend.sum(tf.math.round(keras.backend.clip(\n",
    "        y_true, 0, 1)))\n",
    "    return true_positives / (possible_positives + keras.backend.epsilon())\n",
    "\n",
    "\n",
    "def specificity(y_true, y_pred):\n",
    "    # mask where y_true is nan\n",
    "    is_not_nan = tf.logical_not(tf.math.is_nan(y_true))\n",
    "    y_true = tf.boolean_mask(y_true, is_not_nan)\n",
    "    y_pred = tf.boolean_mask(y_pred, is_not_nan)\n",
    "    # return true negative ratio\n",
    "    true_negatives = keras.backend.sum(tf.math.round(keras.backend.clip(\n",
    "        (1-y_true) * (1-y_pred), 0, 1)))\n",
    "    possible_negatives = keras.backend.sum(tf.math.round(keras.backend.clip(\n",
    "        1-y_true, 0, 1)))\n",
    "    return true_negatives / (possible_negatives + keras.backend.epsilon())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "for gpu in gpus:\n",
    "    tf.config.experimental.set_memory_growth(gpu, True)\n",
    "keras.backend.set_floatx('float32')\n",
    "\n",
    "# load data and reset index\n",
    "data = load_user_data(\"/home/joschi/Documents/Studium/SS19/mu_practical_work/data\")\n",
    "data.reset_index(inplace=True)\n",
    "X, y = split_features_labels(data)\n",
    "attrs = list(X.index)\n",
    "labels = list(y.index)\n",
    "X = X.values\n",
    "y = y.values.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = user_train_test_split(X, y,\n",
    "                                                         test_size=0.2,\n",
    "                                                         random_state=42)\n",
    "X_train, X_valid, y_train, y_valid = user_train_test_split(X_train,\n",
    "                                                           y_train,\n",
    "                                                           test_size=0.25,\n",
    "                                                           random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop uuid column, the timestamps, and the label source\n",
    "X_train = np.delete(X_train, [0, 1, 2, X_train.shape[1] - 1], 1)\n",
    "X_valid = np.delete(X_valid, [0, 1, 2, X_train.shape[1] - 1], 1)\n",
    "X_test = np.delete(X_test, [0, 1, 2, X_test.shape[1] - 1], 1)\n",
    "\n",
    "preprocess_pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy=\"mean\")),\n",
    "    ('std_scaler', StandardScaler()),\n",
    "])\n",
    "X_train = preprocess_pipeline.fit_transform(X_train)\n",
    "X_valid = preprocess_pipeline.transform(X_valid)\n",
    "X_test = preprocess_pipeline.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_nan = np.isnan(y_train)\n",
    "y_train_clean = y_train[~is_nan]\n",
    "class_weights = class_weight.compute_class_weight(\"balanced\",\n",
    "                                                  np.unique(y_train_clean),\n",
    "                                                  y_train_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.astype(np.float32)\n",
    "X_valid = X_valid.astype(np.float32)\n",
    "X_test = X_test.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.AlphaDropout(rate=0.2, \n",
    "                              input_shape=X_train.shape[1:]),\n",
    "    keras.layers.Dense(150, activation=\"selu\",\n",
    "                       kernel_initializer=\"lecun_normal\"),\n",
    "    keras.layers.AlphaDropout(rate=0.2),\n",
    "    keras.layers.Dense(100, activation=\"selu\",\n",
    "                      kernel_initializer=\"lecun_normal\"),\n",
    "    keras.layers.AlphaDropout(rate=0.2),\n",
    "    keras.layers.Dense(100, activation=\"selu\",\n",
    "                      kernel_initializer=\"lecun_normal\"),\n",
    "    keras.layers.AlphaDropout(rate=0.2),\n",
    "    keras.layers.Dense(y_train.shape[1], activation=\"sigmoid\",\n",
    "                       kernel_initializer=\"glorot_uniform\")\n",
    "])\n",
    "\n",
    "gmc_loss = GmcLoss(y_train, alpha=1e-4)\n",
    "optimizer = keras.optimizers.Nadam(lr=0.001)\n",
    "\n",
    "model.compile(loss=gmc_loss, optimizer=optimizer,\n",
    "              metrics=[specificity, sensitivity])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 184596 samples, validate on 46149 samples\n",
      "Epoch 1/100\n",
      "184596/184596 [==============================] - 3s 17us/sample - loss: -0.0517 - specificity: 0.6781 - sensitivity: 0.8147 - val_loss: -0.2370 - val_specificity: 0.7434 - val_sensitivity: 0.8927\n",
      "Epoch 2/100\n",
      "184596/184596 [==============================] - 2s 12us/sample - loss: -0.2182 - specificity: 0.6836 - sensitivity: 0.8799 - val_loss: -0.2648 - val_specificity: 0.7283 - val_sensitivity: 0.9012\n",
      "Epoch 3/100\n",
      "184596/184596 [==============================] - 2s 12us/sample - loss: -0.2301 - specificity: 0.6825 - sensitivity: 0.8850 - val_loss: -0.2283 - val_specificity: 0.7214 - val_sensitivity: 0.9058\n",
      "Epoch 4/100\n",
      "184596/184596 [==============================] - 2s 12us/sample - loss: -0.2377 - specificity: 0.6869 - sensitivity: 0.8874 - val_loss: -0.2102 - val_specificity: 0.7035 - val_sensitivity: 0.9121\n",
      "Epoch 5/100\n",
      "184596/184596 [==============================] - 2s 12us/sample - loss: -0.2431 - specificity: 0.6912 - sensitivity: 0.8880 - val_loss: -0.1985 - val_specificity: 0.6910 - val_sensitivity: 0.9176\n",
      "Epoch 6/100\n",
      "184596/184596 [==============================] - 2s 12us/sample - loss: -0.2472 - specificity: 0.6941 - sensitivity: 0.8904 - val_loss: -0.1924 - val_specificity: 0.6954 - val_sensitivity: 0.9157\n",
      "Epoch 7/100\n",
      "184596/184596 [==============================] - 2s 12us/sample - loss: -0.2503 - specificity: 0.6976 - sensitivity: 0.8914 - val_loss: -0.1946 - val_specificity: 0.7040 - val_sensitivity: 0.9111\n",
      "Epoch 8/100\n",
      "184596/184596 [==============================] - 2s 12us/sample - loss: -0.2535 - specificity: 0.6990 - sensitivity: 0.8924 - val_loss: -0.1999 - val_specificity: 0.7031 - val_sensitivity: 0.9137\n",
      "Epoch 9/100\n",
      "184596/184596 [==============================] - 2s 12us/sample - loss: -0.2561 - specificity: 0.7014 - sensitivity: 0.8930 - val_loss: -0.1849 - val_specificity: 0.6932 - val_sensitivity: 0.9159\n",
      "Epoch 10/100\n",
      "184596/184596 [==============================] - 2s 12us/sample - loss: -0.2582 - specificity: 0.7029 - sensitivity: 0.8935 - val_loss: -0.1921 - val_specificity: 0.6965 - val_sensitivity: 0.9173\n",
      "Epoch 11/100\n",
      "184596/184596 [==============================] - 2s 12us/sample - loss: -0.2599 - specificity: 0.7041 - sensitivity: 0.8938 - val_loss: -0.1898 - val_specificity: 0.6923 - val_sensitivity: 0.9184\n",
      "Epoch 12/100\n",
      "184596/184596 [==============================] - 2s 12us/sample - loss: -0.2613 - specificity: 0.7047 - sensitivity: 0.8949 - val_loss: -0.1809 - val_specificity: 0.6867 - val_sensitivity: 0.9197\n",
      "Epoch 13/100\n",
      "184596/184596 [==============================] - 2s 13us/sample - loss: -0.2626 - specificity: 0.7052 - sensitivity: 0.8954 - val_loss: -0.1975 - val_specificity: 0.7058 - val_sensitivity: 0.9152\n",
      "Epoch 14/100\n",
      "184596/184596 [==============================] - 2s 12us/sample - loss: -0.2641 - specificity: 0.7064 - sensitivity: 0.8955 - val_loss: -0.1837 - val_specificity: 0.6984 - val_sensitivity: 0.9171\n",
      "Epoch 15/100\n",
      "184596/184596 [==============================] - 2s 12us/sample - loss: -0.2650 - specificity: 0.7080 - sensitivity: 0.8958 - val_loss: -0.1778 - val_specificity: 0.6866 - val_sensitivity: 0.9212\n",
      "Epoch 16/100\n",
      "184596/184596 [==============================] - 2s 12us/sample - loss: -0.2665 - specificity: 0.7081 - sensitivity: 0.8966 - val_loss: -0.1743 - val_specificity: 0.6830 - val_sensitivity: 0.9222\n",
      "Epoch 17/100\n",
      "184596/184596 [==============================] - 2s 12us/sample - loss: -0.2678 - specificity: 0.7080 - sensitivity: 0.8971 - val_loss: -0.1785 - val_specificity: 0.6985 - val_sensitivity: 0.9173\n",
      "Epoch 18/100\n",
      "184596/184596 [==============================] - 2s 12us/sample - loss: -0.2686 - specificity: 0.7094 - sensitivity: 0.8973 - val_loss: -0.1665 - val_specificity: 0.6950 - val_sensitivity: 0.9181\n",
      "Epoch 19/100\n",
      "184596/184596 [==============================] - 2s 12us/sample - loss: -0.2694 - specificity: 0.7097 - sensitivity: 0.8973 - val_loss: -0.1720 - val_specificity: 0.7005 - val_sensitivity: 0.9144\n",
      "Epoch 20/100\n",
      "184596/184596 [==============================] - 2s 12us/sample - loss: -0.2706 - specificity: 0.7102 - sensitivity: 0.8976 - val_loss: -0.1682 - val_specificity: 0.6989 - val_sensitivity: 0.9166\n",
      "Epoch 21/100\n",
      "184596/184596 [==============================] - 2s 12us/sample - loss: -0.2713 - specificity: 0.7106 - sensitivity: 0.8982 - val_loss: -0.1611 - val_specificity: 0.6846 - val_sensitivity: 0.9203\n",
      "Epoch 22/100\n",
      "184596/184596 [==============================] - 2s 12us/sample - loss: -0.2721 - specificity: 0.7111 - sensitivity: 0.8980 - val_loss: -0.1586 - val_specificity: 0.6785 - val_sensitivity: 0.9205\n",
      "Epoch 23/100\n",
      "184596/184596 [==============================] - 2s 12us/sample - loss: -0.2730 - specificity: 0.7112 - sensitivity: 0.8981 - val_loss: -0.1554 - val_specificity: 0.6769 - val_sensitivity: 0.9233\n",
      "Epoch 24/100\n",
      "184596/184596 [==============================] - 2s 12us/sample - loss: -0.2735 - specificity: 0.7110 - sensitivity: 0.8986 - val_loss: -0.1553 - val_specificity: 0.6870 - val_sensitivity: 0.9202\n",
      "Epoch 25/100\n",
      "184596/184596 [==============================] - 2s 12us/sample - loss: -0.2744 - specificity: 0.7117 - sensitivity: 0.8989 - val_loss: -0.1470 - val_specificity: 0.6781 - val_sensitivity: 0.9229\n",
      "Epoch 26/100\n",
      "184596/184596 [==============================] - 2s 12us/sample - loss: -0.2750 - specificity: 0.7118 - sensitivity: 0.8987 - val_loss: -0.1515 - val_specificity: 0.6767 - val_sensitivity: 0.9220\n",
      "Epoch 27/100\n",
      "184596/184596 [==============================] - 2s 12us/sample - loss: -0.2757 - specificity: 0.7117 - sensitivity: 0.8997 - val_loss: -0.1487 - val_specificity: 0.6818 - val_sensitivity: 0.9192\n",
      "Epoch 28/100\n",
      "184596/184596 [==============================] - 2s 12us/sample - loss: -0.2759 - specificity: 0.7118 - sensitivity: 0.8992 - val_loss: -0.1455 - val_specificity: 0.6845 - val_sensitivity: 0.9207\n",
      "Epoch 29/100\n",
      "184596/184596 [==============================] - 2s 13us/sample - loss: -0.2768 - specificity: 0.7122 - sensitivity: 0.8996 - val_loss: -0.1493 - val_specificity: 0.6960 - val_sensitivity: 0.9134\n",
      "Epoch 30/100\n",
      "184596/184596 [==============================] - 2s 12us/sample - loss: -0.2774 - specificity: 0.7126 - sensitivity: 0.8996 - val_loss: -0.1498 - val_specificity: 0.6867 - val_sensitivity: 0.9181\n",
      "Epoch 31/100\n",
      "184596/184596 [==============================] - 2s 11us/sample - loss: -0.2780 - specificity: 0.7129 - sensitivity: 0.8998 - val_loss: -0.1429 - val_specificity: 0.6821 - val_sensitivity: 0.9213\n",
      "Epoch 32/100\n",
      "184596/184596 [==============================] - 2s 12us/sample - loss: -0.2786 - specificity: 0.7126 - sensitivity: 0.9000 - val_loss: -0.1475 - val_specificity: 0.6930 - val_sensitivity: 0.9181\n",
      "Epoch 33/100\n",
      "184596/184596 [==============================] - 2s 12us/sample - loss: -0.2790 - specificity: 0.7132 - sensitivity: 0.8999 - val_loss: -0.1456 - val_specificity: 0.6789 - val_sensitivity: 0.9201\n",
      "Epoch 34/100\n",
      "184596/184596 [==============================] - 2s 12us/sample - loss: -0.2796 - specificity: 0.7133 - sensitivity: 0.8994 - val_loss: -0.1537 - val_specificity: 0.6909 - val_sensitivity: 0.9189\n",
      "Epoch 35/100\n",
      "184596/184596 [==============================] - 2s 11us/sample - loss: -0.2798 - specificity: 0.7134 - sensitivity: 0.8994 - val_loss: -0.1647 - val_specificity: 0.6887 - val_sensitivity: 0.9200\n",
      "Epoch 36/100\n",
      "184596/184596 [==============================] - 2s 12us/sample - loss: -0.2806 - specificity: 0.7128 - sensitivity: 0.9003 - val_loss: -0.1515 - val_specificity: 0.6947 - val_sensitivity: 0.9171\n",
      "Epoch 37/100\n",
      "184596/184596 [==============================] - 2s 12us/sample - loss: -0.2812 - specificity: 0.7132 - sensitivity: 0.9001 - val_loss: -0.1541 - val_specificity: 0.6923 - val_sensitivity: 0.9177\n",
      "Epoch 38/100\n",
      "184596/184596 [==============================] - 2s 12us/sample - loss: -0.2816 - specificity: 0.7134 - sensitivity: 0.9001 - val_loss: -0.1566 - val_specificity: 0.6939 - val_sensitivity: 0.9168\n",
      "Epoch 39/100\n",
      "184596/184596 [==============================] - 2s 12us/sample - loss: -0.2818 - specificity: 0.7135 - sensitivity: 0.8998 - val_loss: -0.1519 - val_specificity: 0.6841 - val_sensitivity: 0.9197\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 40/100\n",
      "184596/184596 [==============================] - 2s 12us/sample - loss: -0.2823 - specificity: 0.7136 - sensitivity: 0.8999 - val_loss: -0.1540 - val_specificity: 0.6928 - val_sensitivity: 0.9177\n",
      "Epoch 41/100\n",
      "184596/184596 [==============================] - 2s 12us/sample - loss: -0.2831 - specificity: 0.7141 - sensitivity: 0.9000 - val_loss: -0.1579 - val_specificity: 0.6968 - val_sensitivity: 0.9160\n",
      "Epoch 42/100\n",
      "184596/184596 [==============================] - 2s 12us/sample - loss: -0.2836 - specificity: 0.7146 - sensitivity: 0.9000 - val_loss: -0.1478 - val_specificity: 0.6911 - val_sensitivity: 0.9174\n",
      "Epoch 43/100\n",
      "184596/184596 [==============================] - 2s 12us/sample - loss: -0.2838 - specificity: 0.7144 - sensitivity: 0.9003 - val_loss: -0.1515 - val_specificity: 0.6948 - val_sensitivity: 0.9156\n",
      "Epoch 44/100\n",
      "184596/184596 [==============================] - 2s 12us/sample - loss: -0.2842 - specificity: 0.7153 - sensitivity: 0.9000 - val_loss: -0.1468 - val_specificity: 0.6853 - val_sensitivity: 0.9176\n",
      "Epoch 45/100\n",
      "184596/184596 [==============================] - 2s 13us/sample - loss: -0.2844 - specificity: 0.7146 - sensitivity: 0.9002 - val_loss: -0.1489 - val_specificity: 0.6903 - val_sensitivity: 0.9153\n",
      "Epoch 46/100\n",
      "184596/184596 [==============================] - 2s 12us/sample - loss: -0.2849 - specificity: 0.7151 - sensitivity: 0.9003 - val_loss: -0.1478 - val_specificity: 0.6873 - val_sensitivity: 0.9160\n",
      "Epoch 47/100\n",
      "184596/184596 [==============================] - 2s 12us/sample - loss: -0.2853 - specificity: 0.7150 - sensitivity: 0.9004 - val_loss: -0.1390 - val_specificity: 0.6887 - val_sensitivity: 0.9155\n",
      "Epoch 48/100\n",
      "184596/184596 [==============================] - 2s 12us/sample - loss: -0.2859 - specificity: 0.7156 - sensitivity: 0.9001 - val_loss: -0.1413 - val_specificity: 0.6903 - val_sensitivity: 0.9154\n",
      "Epoch 49/100\n",
      "184596/184596 [==============================] - 2s 12us/sample - loss: -0.2863 - specificity: 0.7159 - sensitivity: 0.9005 - val_loss: -0.1462 - val_specificity: 0.6945 - val_sensitivity: 0.9146\n",
      "Epoch 50/100\n",
      "184596/184596 [==============================] - 2s 12us/sample - loss: -0.2865 - specificity: 0.7169 - sensitivity: 0.8998 - val_loss: -0.1500 - val_specificity: 0.6836 - val_sensitivity: 0.9166\n",
      "Epoch 51/100\n",
      "184596/184596 [==============================] - 2s 12us/sample - loss: -0.2867 - specificity: 0.7162 - sensitivity: 0.9001 - val_loss: -0.1510 - val_specificity: 0.6813 - val_sensitivity: 0.9159\n",
      "Epoch 52/100\n",
      "184596/184596 [==============================] - 2s 12us/sample - loss: -0.2869 - specificity: 0.7162 - sensitivity: 0.9002 - val_loss: -0.1469 - val_specificity: 0.6828 - val_sensitivity: 0.9169\n",
      "Epoch 53/100\n",
      "184596/184596 [==============================] - 2s 12us/sample - loss: -0.2876 - specificity: 0.7171 - sensitivity: 0.8995 - val_loss: -0.1505 - val_specificity: 0.6830 - val_sensitivity: 0.9152\n",
      "Epoch 54/100\n",
      "184596/184596 [==============================] - 2s 12us/sample - loss: -0.2877 - specificity: 0.7165 - sensitivity: 0.9005 - val_loss: -0.1486 - val_specificity: 0.6851 - val_sensitivity: 0.9175\n",
      "Epoch 55/100\n",
      "184596/184596 [==============================] - 2s 12us/sample - loss: -0.2879 - specificity: 0.7170 - sensitivity: 0.9001 - val_loss: -0.1441 - val_specificity: 0.6885 - val_sensitivity: 0.9149\n",
      "Epoch 56/100\n",
      "184596/184596 [==============================] - 2s 12us/sample - loss: -0.2880 - specificity: 0.7168 - sensitivity: 0.9000 - val_loss: -0.1457 - val_specificity: 0.6925 - val_sensitivity: 0.9133\n",
      "Epoch 57/100\n",
      "184596/184596 [==============================] - 2s 12us/sample - loss: -0.2887 - specificity: 0.7179 - sensitivity: 0.9003 - val_loss: -0.1439 - val_specificity: 0.6893 - val_sensitivity: 0.9141\n",
      "Epoch 58/100\n",
      "184596/184596 [==============================] - 2s 13us/sample - loss: -0.2887 - specificity: 0.7175 - sensitivity: 0.9000 - val_loss: -0.1375 - val_specificity: 0.6804 - val_sensitivity: 0.9170\n",
      "Epoch 59/100\n",
      "184596/184596 [==============================] - 2s 12us/sample - loss: -0.2891 - specificity: 0.7179 - sensitivity: 0.9001 - val_loss: -0.1520 - val_specificity: 0.6878 - val_sensitivity: 0.9129\n",
      "Epoch 60/100\n",
      "184596/184596 [==============================] - 2s 12us/sample - loss: -0.2893 - specificity: 0.7180 - sensitivity: 0.9000 - val_loss: -0.1399 - val_specificity: 0.6844 - val_sensitivity: 0.9158\n",
      "Epoch 61/100\n",
      "184596/184596 [==============================] - 2s 12us/sample - loss: -0.2897 - specificity: 0.7176 - sensitivity: 0.9004 - val_loss: -0.1400 - val_specificity: 0.6823 - val_sensitivity: 0.9155\n",
      "Epoch 62/100\n",
      "184596/184596 [==============================] - 2s 12us/sample - loss: -0.2901 - specificity: 0.7181 - sensitivity: 0.9007 - val_loss: -0.1427 - val_specificity: 0.6862 - val_sensitivity: 0.9160\n",
      "Epoch 63/100\n",
      "184596/184596 [==============================] - 2s 12us/sample - loss: -0.2904 - specificity: 0.7189 - sensitivity: 0.9002 - val_loss: -0.1424 - val_specificity: 0.6799 - val_sensitivity: 0.9173\n",
      "Epoch 64/100\n",
      "184596/184596 [==============================] - 2s 12us/sample - loss: -0.2906 - specificity: 0.7181 - sensitivity: 0.9010 - val_loss: -0.1447 - val_specificity: 0.6882 - val_sensitivity: 0.9130\n",
      "Epoch 65/100\n",
      "184596/184596 [==============================] - 2s 12us/sample - loss: -0.2905 - specificity: 0.7187 - sensitivity: 0.9001 - val_loss: -0.1409 - val_specificity: 0.6883 - val_sensitivity: 0.9141\n",
      "Epoch 66/100\n",
      "184596/184596 [==============================] - 2s 12us/sample - loss: -0.2914 - specificity: 0.7193 - sensitivity: 0.9002 - val_loss: -0.1414 - val_specificity: 0.6894 - val_sensitivity: 0.9145\n",
      "Epoch 67/100\n",
      "184596/184596 [==============================] - 2s 12us/sample - loss: -0.2913 - specificity: 0.7198 - sensitivity: 0.9006 - val_loss: -0.1454 - val_specificity: 0.6920 - val_sensitivity: 0.9123\n",
      "Epoch 68/100\n",
      "184596/184596 [==============================] - 2s 12us/sample - loss: -0.2919 - specificity: 0.7195 - sensitivity: 0.9007 - val_loss: -0.1411 - val_specificity: 0.6920 - val_sensitivity: 0.9104\n",
      "Epoch 69/100\n",
      "184596/184596 [==============================] - 2s 12us/sample - loss: -0.2919 - specificity: 0.7197 - sensitivity: 0.9004 - val_loss: -0.1316 - val_specificity: 0.6824 - val_sensitivity: 0.9184\n",
      "Epoch 70/100\n",
      "184596/184596 [==============================] - 2s 12us/sample - loss: -0.2924 - specificity: 0.7201 - sensitivity: 0.9006 - val_loss: -0.1449 - val_specificity: 0.6871 - val_sensitivity: 0.9142\n",
      "Epoch 71/100\n",
      "184596/184596 [==============================] - 2s 13us/sample - loss: -0.2925 - specificity: 0.7196 - sensitivity: 0.9010 - val_loss: -0.1426 - val_specificity: 0.6924 - val_sensitivity: 0.9136\n",
      "Epoch 72/100\n",
      "184596/184596 [==============================] - 2s 12us/sample - loss: -0.2926 - specificity: 0.7202 - sensitivity: 0.9007 - val_loss: -0.1332 - val_specificity: 0.6885 - val_sensitivity: 0.9152\n",
      "Epoch 73/100\n",
      "184596/184596 [==============================] - 2s 12us/sample - loss: -0.2929 - specificity: 0.7201 - sensitivity: 0.9007 - val_loss: -0.1418 - val_specificity: 0.6861 - val_sensitivity: 0.9140\n",
      "Epoch 74/100\n",
      "184596/184596 [==============================] - 2s 12us/sample - loss: -0.2933 - specificity: 0.7204 - sensitivity: 0.9011 - val_loss: -0.1374 - val_specificity: 0.6869 - val_sensitivity: 0.9131\n",
      "Epoch 75/100\n",
      "184596/184596 [==============================] - 2s 12us/sample - loss: -0.2936 - specificity: 0.7212 - sensitivity: 0.9003 - val_loss: -0.1432 - val_specificity: 0.6789 - val_sensitivity: 0.9185\n",
      "Epoch 76/100\n",
      "184596/184596 [==============================] - 2s 12us/sample - loss: -0.2937 - specificity: 0.7198 - sensitivity: 0.9010 - val_loss: -0.1413 - val_specificity: 0.6929 - val_sensitivity: 0.9098\n",
      "Epoch 77/100\n",
      "184596/184596 [==============================] - 2s 12us/sample - loss: -0.2939 - specificity: 0.7213 - sensitivity: 0.9007 - val_loss: -0.1419 - val_specificity: 0.6907 - val_sensitivity: 0.9113\n",
      "Epoch 78/100\n",
      "184596/184596 [==============================] - 2s 12us/sample - loss: -0.2941 - specificity: 0.7210 - sensitivity: 0.9007 - val_loss: -0.1317 - val_specificity: 0.6757 - val_sensitivity: 0.9176\n",
      "Epoch 79/100\n",
      "184596/184596 [==============================] - 2s 12us/sample - loss: -0.2944 - specificity: 0.7209 - sensitivity: 0.9013 - val_loss: -0.1470 - val_specificity: 0.6888 - val_sensitivity: 0.9159\n",
      "Epoch 80/100\n",
      "184596/184596 [==============================] - 2s 12us/sample - loss: -0.2946 - specificity: 0.7217 - sensitivity: 0.9006 - val_loss: -0.1426 - val_specificity: 0.6920 - val_sensitivity: 0.9133\n",
      "Epoch 81/100\n",
      "184596/184596 [==============================] - 2s 12us/sample - loss: -0.2948 - specificity: 0.7216 - sensitivity: 0.9008 - val_loss: -0.1419 - val_specificity: 0.6829 - val_sensitivity: 0.9141\n",
      "Epoch 82/100\n",
      "184596/184596 [==============================] - 2s 12us/sample - loss: -0.2951 - specificity: 0.7213 - sensitivity: 0.9011 - val_loss: -0.1347 - val_specificity: 0.6845 - val_sensitivity: 0.9154\n",
      "Epoch 83/100\n",
      "184596/184596 [==============================] - 2s 12us/sample - loss: -0.2952 - specificity: 0.7215 - sensitivity: 0.9010 - val_loss: -0.1457 - val_specificity: 0.6905 - val_sensitivity: 0.9120\n",
      "Epoch 84/100\n",
      "184596/184596 [==============================] - 2s 12us/sample - loss: -0.2952 - specificity: 0.7218 - sensitivity: 0.9007 - val_loss: -0.1424 - val_specificity: 0.6907 - val_sensitivity: 0.9133\n",
      "Epoch 85/100\n",
      "184596/184596 [==============================] - 2s 12us/sample - loss: -0.2955 - specificity: 0.7218 - sensitivity: 0.9011 - val_loss: -0.1418 - val_specificity: 0.6924 - val_sensitivity: 0.9130\n",
      "Epoch 86/100\n",
      "184596/184596 [==============================] - 2s 13us/sample - loss: -0.2957 - specificity: 0.7222 - sensitivity: 0.9008 - val_loss: -0.1442 - val_specificity: 0.6788 - val_sensitivity: 0.9187\n",
      "Epoch 87/100\n",
      "184596/184596 [==============================] - 2s 12us/sample - loss: -0.2962 - specificity: 0.7218 - sensitivity: 0.9013 - val_loss: -0.1476 - val_specificity: 0.6917 - val_sensitivity: 0.9113\n",
      "Epoch 88/100\n",
      "184596/184596 [==============================] - 2s 12us/sample - loss: -0.2959 - specificity: 0.7222 - sensitivity: 0.9004 - val_loss: -0.1285 - val_specificity: 0.6797 - val_sensitivity: 0.9156\n",
      "Epoch 89/100\n",
      "184596/184596 [==============================] - 2s 12us/sample - loss: -0.2964 - specificity: 0.7220 - sensitivity: 0.9009 - val_loss: -0.1370 - val_specificity: 0.6804 - val_sensitivity: 0.9164\n",
      "Epoch 90/100\n",
      "184596/184596 [==============================] - 2s 12us/sample - loss: -0.2963 - specificity: 0.7221 - sensitivity: 0.9011 - val_loss: -0.1379 - val_specificity: 0.6835 - val_sensitivity: 0.9169\n",
      "Epoch 91/100\n",
      "184596/184596 [==============================] - 2s 12us/sample - loss: -0.2969 - specificity: 0.7222 - sensitivity: 0.9013 - val_loss: -0.1479 - val_specificity: 0.6976 - val_sensitivity: 0.9110\n",
      "Epoch 92/100\n",
      "184596/184596 [==============================] - 2s 12us/sample - loss: -0.2966 - specificity: 0.7226 - sensitivity: 0.9009 - val_loss: -0.1439 - val_specificity: 0.6722 - val_sensitivity: 0.9209\n",
      "Epoch 93/100\n",
      "184596/184596 [==============================] - 2s 12us/sample - loss: -0.2969 - specificity: 0.7217 - sensitivity: 0.9011 - val_loss: -0.1457 - val_specificity: 0.6969 - val_sensitivity: 0.9116\n",
      "Epoch 94/100\n",
      "184596/184596 [==============================] - 2s 12us/sample - loss: -0.2970 - specificity: 0.7227 - sensitivity: 0.9010 - val_loss: -0.1325 - val_specificity: 0.6787 - val_sensitivity: 0.9181\n",
      "Epoch 95/100\n",
      "184596/184596 [==============================] - 2s 12us/sample - loss: -0.2973 - specificity: 0.7222 - sensitivity: 0.9017 - val_loss: -0.1379 - val_specificity: 0.6816 - val_sensitivity: 0.9167\n",
      "Epoch 96/100\n",
      "184596/184596 [==============================] - 2s 12us/sample - loss: -0.2976 - specificity: 0.7225 - sensitivity: 0.9003 - val_loss: -0.1417 - val_specificity: 0.6878 - val_sensitivity: 0.9146\n",
      "Epoch 97/100\n",
      "184596/184596 [==============================] - 2s 12us/sample - loss: -0.2977 - specificity: 0.7230 - sensitivity: 0.9012 - val_loss: -0.1432 - val_specificity: 0.6846 - val_sensitivity: 0.9140\n",
      "Epoch 98/100\n",
      "184596/184596 [==============================] - 2s 12us/sample - loss: -0.2979 - specificity: 0.7232 - sensitivity: 0.9014 - val_loss: -0.1316 - val_specificity: 0.6833 - val_sensitivity: 0.9156\n",
      "Epoch 99/100\n",
      "184596/184596 [==============================] - 2s 12us/sample - loss: -0.2979 - specificity: 0.7226 - sensitivity: 0.9011 - val_loss: -0.1348 - val_specificity: 0.6851 - val_sensitivity: 0.9170\n",
      "Epoch 100/100\n",
      "184596/184596 [==============================] - 2s 12us/sample - loss: -0.2982 - specificity: 0.7227 - sensitivity: 0.9013 - val_loss: -0.1348 - val_specificity: 0.6861 - val_sensitivity: 0.9165\n"
     ]
    }
   ],
   "source": [
    "lr_scheduler = keras.callbacks.ReduceLROnPlateau(factor=0.5, patience=5)\n",
    "early_stopping_cb = keras.callbacks.EarlyStopping(patience=10,\n",
    "                                                  restore_best_weights=True)\n",
    "history = model.fit(X_train, y_train, \n",
    "                    class_weight=class_weights,\n",
    "                    epochs=100, batch_size=1024,\n",
    "                    validation_data=(X_valid, y_valid),\n",
    "                    callbacks=[lr_scheduler, early_stopping_cb])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9.3058193e-01 9.9833846e-01 4.1857362e-04 0.0000000e+00 3.2782555e-07\n",
      " 9.2885876e-01 7.9948342e-01 8.5069448e-02 5.5083036e-03 9.9966609e-01\n",
      " 9.9999911e-01 2.7579069e-04 2.7418137e-06 1.1920929e-07 8.9406967e-08\n",
      " 2.0861626e-07 9.3445063e-01 4.0531158e-06 9.8754358e-01 7.7486038e-07\n",
      " 4.0233135e-06 1.4901161e-07 0.0000000e+00 5.9604645e-08 4.8875809e-06\n",
      " 1.6093254e-06 0.0000000e+00 0.0000000e+00 9.7705162e-01 7.6503438e-01\n",
      " 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 8.0129993e-01\n",
      " 9.5763600e-01 3.8137794e-01 1.4853477e-04 2.7716160e-06 8.9406967e-06\n",
      " 0.0000000e+00 8.3446503e-07 7.4505806e-07 0.0000000e+00 7.0060933e-01\n",
      " 9.8983657e-01 3.7070394e-01 1.1479259e-03 9.9622166e-01 5.7381392e-04\n",
      " 7.4502480e-01]\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "print(y_pred[0])\n",
    "y_pred = np.round(y_pred)\n",
    "y_pred_bias = np.round(model.predict(X_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.,  1.,  0., nan, nan,  0., nan,  0.,  1.,  1., nan, nan,  0.,\n",
       "        0.,  0., nan,  0.,  0.,  0., nan,  0.,  0., nan, nan, nan, nan,\n",
       "       nan,  0., nan, nan, nan, nan, nan,  0.,  1., nan,  0.,  0., nan,\n",
       "       nan, nan, nan, nan, nan,  0.,  1., nan,  0.,  1.,  1.,  1.],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 \n",
      " [[    0 36867]\n",
      " [    0 20690]]\n",
      "1 \n",
      " [[    2 32327]\n",
      " [   20 25208]]\n",
      "2 \n",
      " [[52817  1042]\n",
      " [ 2130  1568]]\n",
      "3 \n",
      " [[32172    67]\n",
      " [  272     3]]\n",
      "4 \n",
      " [[21469    96]\n",
      " [  218   147]]\n",
      "5 \n",
      " [[    0 37409]\n",
      " [    0 14551]]\n",
      "6 \n",
      " [[4757   16]\n",
      " [  21    0]]\n",
      "7 \n",
      " [[25119    22]\n",
      " [ 1229    27]]\n",
      "8 \n",
      " [[51507   107]\n",
      " [ 1693    20]]\n",
      "9 \n",
      " [[11037 37910]\n",
      " [ 1018 11295]]\n",
      "10 \n",
      " [[    0  3102]\n",
      " [    0 32464]]\n",
      "11 \n",
      " [[24815  4026]\n",
      " [  604  2498]]\n",
      "12 \n",
      " [[40741   363]\n",
      " [ 1201   596]]\n",
      "13 \n",
      " [[31092     0]\n",
      " [  251     0]]\n",
      "14 \n",
      " [[35869   288]\n",
      " [ 1513   930]]\n",
      "15 \n",
      " [[27002   505]\n",
      " [  192    27]]\n",
      "16 \n",
      " [[    1 42980]\n",
      " [    0 26413]]\n",
      "17 \n",
      " [[25332     3]\n",
      " [  396     3]]\n",
      "18 \n",
      " [[ 3394 26120]\n",
      " [   68  4787]]\n",
      "19 \n",
      " [[48696   337]\n",
      " [  712   225]]\n",
      "20 \n",
      " [[48812     0]\n",
      " [  985     0]]\n",
      "21 \n",
      " [[30387     0]\n",
      " [  422     0]]\n",
      "22 \n",
      " [[13114     0]\n",
      " [   79     0]]\n",
      "23 \n",
      " [[2653    0]\n",
      " [  28    0]]\n",
      "24 \n",
      " [[34987     0]\n",
      " [  471     0]]\n",
      "25 \n",
      " [[27739     0]\n",
      " [  518     0]]\n",
      "26 \n",
      " [[16158     0]\n",
      " [  121     0]]\n",
      "27 \n",
      " [[30978     0]\n",
      " [  603     0]]\n",
      "28 \n",
      " [[17209 17674]\n",
      " [ 2825  1567]]\n",
      "29 \n",
      " [[ 9479 19398]\n",
      " [ 1367  4766]]\n",
      "30 \n",
      " [[17937    36]\n",
      " [  145     0]]\n",
      "31 \n",
      " [[4494    0]\n",
      " [  26    0]]\n",
      "32 \n",
      " []\n",
      "33 \n",
      " [[7116    0]\n",
      " [ 333    0]]\n",
      "34 \n",
      " [[ 7232 42239]\n",
      " [   53  6204]]\n",
      "35 \n",
      " [[ 5461 33971]\n",
      " [   16  7744]]\n",
      "36 \n",
      " [[40421 13801]\n",
      " [ 2437  1188]]\n",
      "37 \n",
      " [[42108     0]\n",
      " [  793     0]]\n",
      "38 \n",
      " [[36301     0]\n",
      " [  590     0]]\n",
      "39 \n",
      " [[28948     0]\n",
      " [  623     0]]\n",
      "40 \n",
      " [[13346     0]\n",
      " [  252     0]]\n",
      "41 \n",
      " [[12579     0]\n",
      " [  285     0]]\n",
      "42 \n",
      " [[27443     0]\n",
      " [  281     0]]\n",
      "43 \n",
      " [[31836     0]\n",
      " [  122     0]]\n",
      "44 \n",
      " [[18941 31315]\n",
      " [ 1463  5838]]\n",
      "45 \n",
      " [[ 8494 34193]\n",
      " [   98  8423]]\n",
      "46 \n",
      " [[ 2852 27848]\n",
      " [  165  2379]]\n",
      "47 \n",
      " [[22211   560]\n",
      " [ 1754   359]]\n",
      "48 \n",
      " [[    0  9512]\n",
      " [    0 26451]]\n",
      "49 \n",
      " [[18770   270]\n",
      " [  754    11]]\n",
      "50 \n",
      " [[ 6504 21730]\n",
      " [  414  3080]]\n",
      "Balanced accuracy: 0.56\n",
      "0 \n",
      " [[     0 123756]\n",
      " [     0  67791]]\n",
      "1 \n",
      " [[  3872 105573]\n",
      " [     7  84282]]\n",
      "2 \n",
      " [[176526   4141]\n",
      " [  6833   6234]]\n",
      "3 \n",
      " [[95355   228]\n",
      " [  139   391]]\n",
      "4 \n",
      " [[88640   484]\n",
      " [ 1273  2969]]\n",
      "5 \n",
      " [[     0 127951]\n",
      " [     0  55031]]\n",
      "6 \n",
      " [[34911  2828]\n",
      " [  853  2661]]\n",
      "7 \n",
      " [[57835  3860]\n",
      " [ 1583  2206]]\n",
      "8 \n",
      " [[130468    922]\n",
      " [  1579    777]]\n",
      "9 \n",
      " [[60153 17030]\n",
      " [ 1948 12487]]\n",
      "10 \n",
      " [[    22   6890]\n",
      " [     0 114165]]\n",
      "11 \n",
      " [[82222  2247]\n",
      " [ 1483  5429]]\n",
      "12 \n",
      " [[92691  1590]\n",
      " [  948  2132]]\n",
      "13 \n",
      " [[105954    248]\n",
      " [  1125    110]]\n",
      "14 \n",
      " [[89465  1948]\n",
      " [ 1016  2316]]\n",
      "15 \n",
      " [[81034  1193]\n",
      " [  768  1409]]\n",
      "16 \n",
      " [[    15 122306]\n",
      " [     0  94721]]\n",
      "17 \n",
      " [[89569   308]\n",
      " [  662   222]]\n",
      "18 \n",
      " [[45810 23105]\n",
      " [ 2559 13618]]\n",
      "19 \n",
      " [[150885    892]\n",
      " [  2200   3443]]\n",
      "20 \n",
      " [[128457    432]\n",
      " [  1973    162]]\n",
      "21 \n",
      " [[77761   357]\n",
      " [  789   120]]\n",
      "22 \n",
      " [[29072   608]\n",
      " [  210   153]]\n",
      "23 \n",
      " [[53258   677]\n",
      " [  496   735]]\n",
      "24 \n",
      " [[131537     92]\n",
      " [  1347     83]]\n",
      "25 \n",
      " [[106181    837]\n",
      " [  1603    595]]\n",
      "26 \n",
      " [[43156   661]\n",
      " [  187    95]]\n",
      "27 \n",
      " [[92193     1]\n",
      " [  457     0]]\n",
      "28 \n",
      " [[83547 26995]\n",
      " [ 2879  3296]]\n",
      "29 \n",
      " [[86281 36668]\n",
      " [ 2259  8887]]\n",
      "30 \n",
      " [[25287   424]\n",
      " [  215   813]]\n",
      "31 \n",
      " [[10500    28]\n",
      " [   34   117]]\n",
      "32 \n",
      " [[38399   177]\n",
      " [  232   230]]\n",
      "33 \n",
      " [[13278   189]\n",
      " [  150   136]]\n",
      "34 \n",
      " [[ 50163 108735]\n",
      " [   434  20640]]\n",
      "35 \n",
      " [[44637 82333]\n",
      " [  299 24342]]\n",
      "36 \n",
      " [[153588  30900]\n",
      " [  6629   3671]]\n",
      "37 \n",
      " [[142356      0]\n",
      " [  1677      0]]\n",
      "38 \n",
      " [[122839    195]\n",
      " [  1677     79]]\n",
      "39 \n",
      " [[152495    229]\n",
      " [  1304    117]]\n",
      "40 \n",
      " [[16483    82]\n",
      " [  100   238]]\n",
      "41 \n",
      " [[49296   105]\n",
      " [  181   134]]\n",
      "42 \n",
      " [[49552    82]\n",
      " [  162   129]]\n",
      "43 \n",
      " [[33108     0]\n",
      " [   70     0]]\n",
      "44 \n",
      " [[99708 70211]\n",
      " [ 4156 19659]]\n",
      "45 \n",
      " [[57719 80907]\n",
      " [  193 24212]]\n",
      "46 \n",
      " [[56539 17404]\n",
      " [ 2684  6450]]\n",
      "47 \n",
      " [[42363  2597]\n",
      " [ 1606  3585]]\n",
      "48 \n",
      " [[  471 29242]\n",
      " [    0 71257]]\n",
      "49 \n",
      " [[53211  2710]\n",
      " [ 1201  1537]]\n",
      "50 \n",
      " [[48364 34906]\n",
      " [ 1851 14794]]\n",
      "Balanced accuracy bias: 0.70\n"
     ]
    }
   ],
   "source": [
    "from metrics import balanced_accuracy_score\n",
    "\n",
    "ba_score = balanced_accuracy_score(y_test.T, y_pred.T, average=\"macro\")\n",
    "print(\"Balanced accuracy: {:.2f}\".format(ba_score))\n",
    "ba_bias_score = balanced_accuracy_score(y_train.T, y_pred_bias.T, average=\"macro\")\n",
    "print(\"Balanced accuracy bias: {:.2f}\".format(ba_bias_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
